{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2idx = {'SOS' : SOS_TOKEN, 'EOS' : EOS_TOKEN}\n",
    "        self.idx2word = {v : k for k,v in self.word2idx.items()}\n",
    "        self.word2cnt = {}\n",
    "        self.idx = 3\n",
    "        \n",
    "    def add_word(self, w):\n",
    "        if w not in self.word2idx:\n",
    "            self.word2idx[w] = self.idx\n",
    "            self.idx2word[self.idx] = w\n",
    "            self.word2cnt[w] = 1\n",
    "            self.idx += 1\n",
    "        else:\n",
    "            self.word2cnt[w] += 1\n",
    "    \n",
    "    def add_sentence(self, s):\n",
    "#         s = s.lower().strip()    # just to be sure\n",
    "        for w in s.split(' '):\n",
    "            self.add_word(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):                                      #? why convert 2 ascii\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'addas .  !  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'addas.!?'\n",
    "x = re.sub(r\"[^A-Za-z0-9,.!\\/'+=]\", \" \", x)\n",
    "x = re.sub(r\"\\.\", r\" . \", x)\n",
    "x = re.sub(r\"!\", r\" ! \", x)\n",
    "x = re.sub(r\"\\?\", r\" ? \", x)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    if s[-1] in ['.', '?', '!']:\n",
    "        s = s[:-1] + ' ' + s[-1]\n",
    "    \n",
    "#     s = s.lower().strip()\n",
    "    s = re.sub(r\"[^A-Za-z0-9,.!\\?\\/'+=]\", \" \", s)\n",
    "    s = re.sub(r\"!\", \" ! \", s)\n",
    "    s = re.sub(r\"\\.\", \" . \", s)\n",
    "#     s = re.sub(r\"\\?\", \" ? \", s)\n",
    "    s = re.sub(r\"what's\", \"what is \", s)\n",
    "    s = re.sub(r\"it's\", r\"it is\", s)\n",
    "    s = re.sub(r\"\\'s\", \" \", s)\n",
    "    s = re.sub(r\" n \", \" and \", s)\n",
    "    s = re.sub(r\"\\'ve\", \" have \", s)\n",
    "    s = re.sub(r\"can't\", \"cannot \", s)\n",
    "    s = re.sub(r\"n't\", \" not \", s)\n",
    "    s = re.sub(r\"i'm\", \"i am \", s)\n",
    "    s = re.sub(r\"\\'re\", \" are \", s)\n",
    "    s = re.sub(r\"\\'d\", \" would \", s)\n",
    "    s = re.sub(r\"\\'ll\", \" will \", s)\n",
    "    s = re.sub(r\",\", \" \", s)\n",
    "    s = re.sub(r\"\\/\", \" \", s)\n",
    "    s = re.sub(r\"\\^\", \" ^ \", s)\n",
    "    s = re.sub(r\"\\+\", \" + \", s)\n",
    "    s = re.sub(r\"\\-\", \" - \", s)\n",
    "    s = re.sub(r\"\\=\", \" = \", s)\n",
    "    s = re.sub(r\"'\", \" \", s)\n",
    "    s = re.sub(r\":\", \" : \", s)\n",
    "    s = re.sub(' +', ' ', s)\n",
    "    \n",
    "    s = s.strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'when i was a kid touching bugs did not bother me a bit . now i can hardly stand looking at pictures of them .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'When I was a kid, touching bugs didn\\'t bother me a bit. Now I can hardly stand looking at pictures of them.'\n",
    "normalize_string(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indent_end(s):\n",
    "    if s[-1] in ['|', '?', '!', '।']:\n",
    "        s = s[:-1] + ' ' + s[-1]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse = False):\n",
    "    data = io.open(f'{lang1}-{lang2}/{lang1}.txt').read().strip().split('\\n')\n",
    "    pairs = [[normalize_string(s) for s in line.split('\\t')] for line in data]\n",
    "    pairs = []\n",
    "    for line in data:\n",
    "        s = line.split('\\t')\n",
    "        pairs.append([normalize_string(s[0]), indent_end(s[1])])\n",
    "    \n",
    "    if reverse:\n",
    "        pairs = [list(reversed(pair)) for pair in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2867 sentence pairs\n",
      "['wooden buildings catch fire easily .', 'लकड़ी से बनी इमारतों में आग आसानी से लग जाती है ।']\n",
      "['i am bored .', 'मैं बोर हो रहा हूँ ।']\n",
      "Counting words...\n",
      "hin 2364\n",
      "eng 2970\n",
      "['the air conditioner makes too much noise .', 'एसी बहुत आवाज़ करता है ।']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1, lang2, reverse = False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1, lang2, reverse)\n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    print(random.choice(pairs))\n",
    "    print(random.choice(pairs))\n",
    "    \n",
    "    print(\"Counting words...\")\n",
    "    print(input_lang.name, input_lang.idx)\n",
    "    print(output_lang.name, output_lang.idx)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('hin', 'eng', False)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = max(len(pair[0]) for pair in pairs)\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ट्रेफ़िक', 2960),\n",
       " ('हादसे', 2961),\n",
       " ('कीड़ों', 2962),\n",
       " ('परेशानी', 2963),\n",
       " ('जिस', 2964),\n",
       " ('करें।', 2965),\n",
       " ('दूसरी', 2966),\n",
       " ('अनुवादों', 2967),\n",
       " ('प्रभावित', 2968),\n",
       " ('दें', 2969)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(output_lang.word2idx.items(), key = lambda x : x[1])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 2354),\n",
       " ('form', 2355),\n",
       " ('kid', 2356),\n",
       " ('touching', 2357),\n",
       " ('bugs', 2358),\n",
       " ('bother', 2359),\n",
       " ('translation', 2360),\n",
       " ('translating', 2361),\n",
       " ('translations', 2362),\n",
       " ('influence', 2363)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(input_lang.word2idx.items(), key = lambda x : x[1])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SOS', 1),\n",
       " ('EOS', 2),\n",
       " ('wow', 3),\n",
       " ('!', 4),\n",
       " ('help', 5),\n",
       " ('jump', 6),\n",
       " ('.', 7),\n",
       " ('hello', 8),\n",
       " ('cheers', 9),\n",
       " ('got', 10)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(input_lang.word2idx.items(), key = lambda x : x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('not', 298),\n",
       " ('he', 393),\n",
       " ('a', 418),\n",
       " ('?', 445),\n",
       " ('is', 504),\n",
       " ('you', 544),\n",
       " ('to', 553),\n",
       " ('i', 748),\n",
       " ('the', 792),\n",
       " ('.', 2404)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(input_lang.word2cnt.items(), key = lambda x : x[1])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wow', 1),\n",
       " ('awesome', 1),\n",
       " ('goodbye', 1),\n",
       " ('full', 1),\n",
       " ('fantastic', 1),\n",
       " ('fainted', 1),\n",
       " ('fear', 1),\n",
       " ('definitely', 1),\n",
       " ('burns', 1),\n",
       " ('easter', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(input_lang.word2cnt.items(), key = lambda x : x[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size = input_lang.idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hin_vocab_size = output_lang.idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2365, 2971)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_vocab_size, hin_vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](encoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, n_layers):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        seq_len = len(x)\n",
    "        embedded = nn.Embedding(x).view(1, seq_len, -1)\n",
    "        \n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        hidden = torch.zeros(self.n_layers, 1, self.hidden_size)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](AttnDecoderRNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the Luong et al. model(s)\n",
    "Effective Approaches to Attention-based Neural Machine Translation by Luong et al. describe a few more attention models that offer improvements and simplifications. They describe a few \"global attention\" models, the distinction between them being the way the attention scores are calculated.\n",
    "\n",
    "The general form of the attention calculation relies on the target (decoder) side hidden state and corresponding source (encoder) side state, normalized over all states to get values summing to 1:\n",
    "\n",
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "The specific \"score\" function that compares two states is either dot, a simple dot product between the states; general, a a dot product between the decoder hidden state and a linear transform of the encoder state; or concat, a dot product between a new parameter $v_a$ and a linear transform of the states concatenated together.\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) =\n",
    "\\begin{cases}\n",
    "h_t ^\\top \\bar h_s &amp; dot \\\\\n",
    "h_t ^\\top \\textbf{W}_a \\bar h_s &amp; general \\\\\n",
    "v_a ^\\top \\textbf{W}_a [ h_t ; \\bar h_s ] &amp; concat\n",
    "\\end{cases}\n",
    "$$\n",
    "The modular definition of these scoring functions gives us an opportunity to build specific attention module that can switch between the different score methods. The input to this module is always the hidden state (of the decoder RNN) and set of encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size, method = 'concat', max_length = MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.method = method\n",
    "        \n",
    "        self.attn = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdev = 1./math.sqrt(self.v.size(0)) \n",
    "        self.v.data.normal_(mean = 0, std = stdev)\n",
    "        \n",
    "#     def forward(self, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.rand(1, 256)\n",
    "enc = torch.rand(127, 1, 256)\n",
    "# torch.cat((hidden, enc), dim = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 127, 256])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = hidden.repeat(127, 1, 1).transpose(0, 1)\n",
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 127, 256])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_trans = enc.transpose(0, 1)\n",
    "enc_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 127, 512])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((H, enc_trans), dim = 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, hidden_size, method = 'concat', max_length = MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
    "            \n",
    "        if self.method == 'concat':\n",
    "            self.attn = nn.Linear(2 * hidden_size, hidden_size)\n",
    "            self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "            stdev = 1. / math.sqrt(self.v.size(0))\n",
    "            self.v.data.normal_(mean = 0, std = stdev)\n",
    "            \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        '''\n",
    "        :param hidden: \n",
    "            previous hidden state of the decoder, in shape (B, H) as hidden[-1]\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs from Encoder, in shape (T, B, H)\n",
    "        :return\n",
    "            attention wts in shape (B, T)\n",
    "        '''\n",
    "        # B = 1 (for us)\n",
    "        \n",
    "        H = hidden.repeat(self.max_length, 1, 1).transpose(0, 1)        # H -> [T, B, H], after transpose [B, T, H]\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)               # encoder_outputs -> [B, T, H]\n",
    "        attn_energies = self.score(H, encoder_outputs)                  # compute attention score\n",
    "        return F.softmax(attn_energies).unsqueeze(1)                    # normalize with softmax\n",
    "    \n",
    "    def score(self, hidden, encoder_outputs):         \n",
    "        if self.method == 'dot':         #??NOT WORKING\n",
    "            energy = hidden.dot(encoder_output)     # hidden -> [n_layers * directional, B, H], encoder_output -> [T, B, H]\n",
    "            return energy\n",
    "        \n",
    "        if self.method == 'general':     #??NOT WORKING\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "            \n",
    "        if self.method == 'concat':\n",
    "            energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2))) # [B, T, 2H] -> [B, T, H]\n",
    "            energy = energy.transpose(2,1)       # [B, H, T]\n",
    "            v = self.v.repeat(encoder_outputs.data.shape[0], 1).unsqueeze(1)    #[B, 1, H]\n",
    "            energy = torch.bmm(v, energy)        # [B, 1, T]\n",
    "            return energy.squeeze(1)             # [B, T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](BahdanauAttention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers, dropout_p, max_length = MAX_LENGTH):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(p = dropout_p)\n",
    "        self.attn = Attn(hidden_size, method = 'concat')\n",
    "#         self.attn = nn.Linear(2 * hidden_size, max_length)\n",
    "        self.attn_combine = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, encoder_outputs):             # encoder_outputs => (max_length, encoder.hidden_size)\n",
    "        '''\n",
    "        :param word_input:\n",
    "            word input for current time step, in shape (B)\n",
    "        :param last_hidden:\n",
    "            last hidden stat of the decoder, in shape (layers*direction, B, H)\n",
    "        :param encoder_outputs:\n",
    "            encoder outputs in shape (T, B, H)\n",
    "        :return\n",
    "            decoder output\n",
    "        Note: we run this one step at a time i.e. you should use a outer loop \n",
    "            to process the whole sequence\n",
    "        Tip(update):\n",
    "        EncoderRNN may be bidirectional or have multiple layers, so the shape of hidden states can be \n",
    "        different from that of DecoderRNN\n",
    "        You may have to manually guarantee that they have the same dimension outside this function,\n",
    "        e.g, select the encoder hidden state of the foward/backward pass.\n",
    "        '''\n",
    "        embedded = self.embedding(x).view(1, 1, -1)              #  (1, B, V)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "#         attn_weights = F.softmax(self.attn(torch.cat((hidden[-1], encoder_outputs), dim = 1)), dim = 1)\n",
    "        attn_weights = self.attn(hidden[-1], encoder_outputs)        # attn_weights -> [B, T]\n",
    "    \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))          # context -> [B, 1, V]\n",
    "        context = context.transpose(0, 1)                                    # context -> [1, B, V]\n",
    "         \n",
    "        attn_combine = self.attn_combine(torch.cat((embedded, context), dim = 2))\n",
    "        output = F.relu(attn_combine)\n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        output = F.log_softmax(self.output(output[0]), dim = 1)\n",
    "        \n",
    "        return output, hidden, attn_weights\n",
    "        \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2idx[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensor_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_TOKEN)\n",
    "    return torch.tensor(indexes, dtype = torch.long).view(-1, 1)\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    input_tensor = tensor_from_sentence(input_lang, pair[0])\n",
    "    target_tensor = tensor_from_sentence(output_lang, pair[1])\n",
    "    \n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3],\n",
       "         [4],\n",
       "         [2]]), tensor([[3],\n",
       "         [4],\n",
       "         [2]]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors_from_pair(pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Teacher forcing** is the concept of using the real target outputs as each next input, instead of using the decoder’s guess as the next input. Using teacher forcing causes it to converge faster but when the trained network is exploited, it may exhibit instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.6\n",
    "clip = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length = MAX_LENGTH):    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, 1, encoder.hidden_size)\n",
    "    \n",
    "#     for ei in range(input_length):\n",
    "#         encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "#         encoder_outputs[ei] = encoder_output[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "    encoder_outputs[:encoder_output.size(0), :, :] = encoder_output\n",
    "#     print(encoder_outputs.shape)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_TOKEN]], dtype = torch.long)\n",
    "    \n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            decoder_input = target_tensor[di]\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "        else:\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()             # detach from history as input\n",
    "            \n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "        \n",
    "        if decoder_input.item() == EOS_TOKEN:\n",
    "            break\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm = clip)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), max_norm = clip)    \n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a helper function to print time elapsed and estimated time remaining given the current time and progress %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The whole training process looks like this:\n",
    "* Start a timer\n",
    "* Initialize optimizers and criterion\n",
    "* Create set of training pairs\n",
    "* Start empty losses array for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iters(encoder, decoder, n_iters, print_every = 1000, plot_every = 100, learning_rate = 0.001):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset after every print_every\n",
    "    plot_loss_total = 0  # Reset after every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr = learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr = learning_rate)\n",
    "    \n",
    "    training_pairs = [tensors_from_pair(random.choice(pairs)) for _ in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for n_iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[n_iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        plot_losses.append(loss)\n",
    "        \n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if n_iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, n_iter / n_iters),\n",
    "                                         n_iter, n_iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if n_iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length = MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensor_from_sentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, 1, encoder.hidden_size)\n",
    "\n",
    "#         for ei in range(input_length):\n",
    "#             encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "#                                                      encoder_hidden)\n",
    "#             encoder_outputs[ei] += encoder_output[0, 0]\n",
    "        \n",
    "        # Run words through encoder\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
    "        encoder_outputs[:encoder_output.size(0), :, :] = encoder_output\n",
    "    \n",
    "        decoder_input = torch.tensor([[SOS_TOKEN]])  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_TOKEN:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.idx2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 36s (- 159m 30s) (1000 1%) 4.9106\n",
      "3m 31s (- 172m 19s) (2000 2%) 4.3582\n",
      "5m 18s (- 171m 52s) (3000 3%) 3.8928\n",
      "7m 7s (- 171m 4s) (4000 4%) 3.3771\n",
      "9m 5s (- 172m 45s) (5000 5%) 3.1241\n",
      "11m 8s (- 174m 31s) (6000 6%) 2.7709\n",
      "13m 21s (- 177m 23s) (7000 7%) 2.4828\n",
      "15m 22s (- 176m 51s) (8000 8%) 2.3512\n",
      "17m 19s (- 175m 11s) (9000 9%) 2.0611\n",
      "19m 19s (- 173m 51s) (10000 10%) 1.9179\n",
      "21m 16s (- 172m 9s) (11000 11%) 1.7741\n",
      "23m 15s (- 170m 36s) (12000 12%) 1.6709\n",
      "25m 14s (- 168m 52s) (13000 13%) 1.5095\n",
      "27m 12s (- 167m 9s) (14000 14%) 1.5181\n",
      "29m 11s (- 165m 27s) (15000 15%) 1.2247\n",
      "31m 10s (- 163m 40s) (16000 16%) 1.3195\n",
      "33m 16s (- 162m 26s) (17000 17%) 1.1914\n",
      "35m 15s (- 160m 36s) (18000 18%) 1.0954\n",
      "37m 14s (- 158m 47s) (19000 19%) 1.0362\n",
      "39m 14s (- 156m 57s) (20000 20%) 0.9979\n",
      "41m 14s (- 155m 8s) (21000 21%) 0.9994\n",
      "43m 14s (- 153m 17s) (22000 22%) 0.9606\n",
      "45m 14s (- 151m 26s) (23000 23%) 0.9733\n",
      "47m 13s (- 149m 34s) (24000 24%) 0.9143\n",
      "49m 14s (- 147m 42s) (25000 25%) 0.8312\n",
      "51m 13s (- 145m 49s) (26000 26%) 0.8500\n",
      "53m 13s (- 143m 54s) (27000 27%) 0.7436\n",
      "55m 12s (- 141m 58s) (28000 28%) 0.7375\n",
      "57m 11s (- 140m 2s) (29000 28%) 0.7693\n",
      "59m 10s (- 138m 3s) (30000 30%) 0.7241\n",
      "61m 8s (- 136m 6s) (31000 31%) 0.7478\n",
      "63m 7s (- 134m 8s) (32000 32%) 0.6748\n",
      "65m 6s (- 132m 10s) (33000 33%) 0.6786\n",
      "67m 3s (- 130m 10s) (34000 34%) 0.6511\n",
      "69m 0s (- 128m 9s) (35000 35%) 0.6043\n",
      "70m 58s (- 126m 11s) (36000 36%) 0.6303\n",
      "72m 56s (- 124m 11s) (37000 37%) 0.6565\n",
      "74m 53s (- 122m 11s) (38000 38%) 0.5937\n",
      "76m 50s (- 120m 11s) (39000 39%) 0.5644\n",
      "78m 48s (- 118m 13s) (40000 40%) 0.6175\n",
      "80m 46s (- 116m 13s) (41000 41%) 0.5704\n",
      "82m 43s (- 114m 14s) (42000 42%) 0.5553\n",
      "84m 40s (- 112m 14s) (43000 43%) 0.5453\n",
      "86m 38s (- 110m 16s) (44000 44%) 0.5943\n",
      "88m 36s (- 108m 17s) (45000 45%) 0.5354\n",
      "90m 33s (- 106m 18s) (46000 46%) 0.5189\n",
      "92m 30s (- 104m 19s) (47000 47%) 0.5630\n",
      "94m 29s (- 102m 22s) (48000 48%) 0.5746\n",
      "96m 27s (- 100m 23s) (49000 49%) 0.6056\n",
      "98m 24s (- 98m 24s) (50000 50%) 0.5258\n",
      "100m 21s (- 96m 25s) (51000 51%) 0.5189\n",
      "102m 20s (- 94m 27s) (52000 52%) 0.5270\n",
      "104m 17s (- 92m 29s) (53000 53%) 0.5254\n",
      "106m 16s (- 90m 31s) (54000 54%) 0.4921\n",
      "108m 15s (- 88m 34s) (55000 55%) 0.4277\n",
      "110m 15s (- 86m 38s) (56000 56%) 0.4568\n",
      "130m 33s (- 98m 29s) (57000 56%) 0.6140\n",
      "133m 6s (- 96m 23s) (58000 57%) 0.5245\n",
      "135m 7s (- 93m 54s) (59000 59%) 0.5339\n",
      "137m 14s (- 91m 29s) (60000 60%) 0.5183\n",
      "139m 32s (- 89m 13s) (61000 61%) 0.5282\n",
      "141m 55s (- 86m 59s) (62000 62%) 0.5639\n",
      "144m 6s (- 84m 38s) (63000 63%) 0.5429\n",
      "146m 11s (- 82m 14s) (64000 64%) 0.4711\n",
      "148m 27s (- 79m 56s) (65000 65%) 0.4720\n",
      "150m 38s (- 77m 36s) (66000 66%) 0.5071\n",
      "153m 8s (- 75m 25s) (67000 67%) 0.4397\n",
      "155m 23s (- 73m 7s) (68000 68%) 0.4546\n",
      "157m 40s (- 70m 50s) (69000 69%) 0.5052\n",
      "159m 48s (- 68m 29s) (70000 70%) 0.4763\n",
      "161m 58s (- 66m 9s) (71000 71%) 0.4766\n",
      "164m 5s (- 63m 48s) (72000 72%) 0.4624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-068b2362bf03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattn_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBahdanauAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhin_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-133-1a7a3ca2b143>\u001b[0m in \u001b[0;36mtrain_iters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 20\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mplot_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-182-73881fefae50>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder = EncoderRNN(eng_vocab_size, hidden_size, 1)\n",
    "attn_decoder = BahdanauAttnDecoderRNN(hidden_size, hin_vocab_size, 1, dropout_p = 0.1)\n",
    "\n",
    "train_iters(encoder, attn_decoder, 100000, print_every = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> as soon as he finished his work he went home .\n",
      "= वह अपना काम खतम करते ही घर चला गया ।\n",
      "< वह अपना काम खतम करते ही घर खतम हो चला गया । <EOS>\n",
      "\n",
      "> i doubt the truth of his statement .\n",
      "= मुझे उसके बयान की सच्चाई पर शक है ।\n",
      "< मुझे उसके बयान की सच्चाई पर शक है । <EOS>\n",
      "\n",
      "> he may be rich but he is stingy .\n",
      "= वह अमीर है हो है पर कंजूस भी है ।\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< वह अमीर है है है है है है है है है है है है है है है है है है । <EOS>\n",
      "\n",
      "> i would rather not meet him .\n",
      "= हो सके तो मैं उससे मिलना नहीं चाहूँगा ।\n",
      "< हो सके तो मैं उसे मिलना नहीं चाहूँगी । <EOS>\n",
      "\n",
      "> there are many old temples in kyoto .\n",
      "= क्योटो में बहुत सारे पुराने मंदिर हैं ।\n",
      "< क्योटो में बहुत सारे पुराने मंदिर हैं । <EOS>\n",
      "\n",
      "> cannot you speak english ?\n",
      "= तुम अंग्रेज़ी नहीं बोल सकते हो क्या ?\n",
      "< तुम अंग्रेज़ी नहीं बोल सकते हो क्या ? <EOS>\n",
      "\n",
      "> i fear so .\n",
      "= खेद की बात है, लेकिन वैसा ही है ।\n",
      "< मैं वैसा बात क्यों नहीं हूँ । <EOS>\n",
      "\n",
      "> i work for a bank .\n",
      "= मैं बैंक में काम करता हूँ ।\n",
      "< मैं बैंक में काम करता हूँ । <EOS>\n",
      "\n",
      "> my father spends a lot of time on his hobby .\n",
      "= मेरे पिता अपने शौक पर बहुत समय खर्च करते हैं ।\n",
      "< मेरे पिता अपने शौक के साथ शौक मत भूलना । <EOS>\n",
      "\n",
      "> here please have a seat .\n",
      "= यहां, बैठिये ।\n",
      "< यहां, बैठिये । <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder, attn_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbbe4219780>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder, attn_decoder, \"i want a drink .\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = wow !\n",
      "output = बधाई ! <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"wow !\")\n",
    "\n",
    "# evaluateAndShowAttention(\"elle est trop petit .\")\n",
    "\n",
    "# evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
    "\n",
    "# evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "वाह मेरी बात कीजिए । <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "s = input('Enter sentence :: ')\n",
    "output_words, attentions = evaluate(encoder, attn_decoder, normalize_string(s))\n",
    "print(' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
